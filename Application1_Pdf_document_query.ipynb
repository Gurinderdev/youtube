{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install PyPDF2\n",
    "# !pip install faiss-cpu\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a page on gitmind for this \n",
    " 1.read the pdfs\n",
    "2. Break the pdfs into components or blocks\n",
    "3. Create embedings \n",
    "4. Use a database for the embedings - Vectorstore\n",
    "'''\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"add your key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader(r'..\\fake_cv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes: we need to read in all the pages \n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(model= 'gpt-3.5-turbo-instruct'), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" how many companies worked in. and for how many years \"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the python libraries you know \"\n",
    "docs = document_search.similarity_search(query)\n",
    "print(chain.run(input_documents=docs, question=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_pdf(pdf_file ):\n",
    "    #step 1\n",
    "    pdfreader = PdfReader(pdf_file)\n",
    "    #step 2\n",
    "    # read text from pdf\n",
    "    raw_text = ''\n",
    "    for i, page in enumerate(pdfreader.pages):\n",
    "        content = page.extract_text()\n",
    "        if content:\n",
    "            raw_text += content\n",
    "    #step 3\n",
    "    # We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator = \"\\n\",\n",
    "        chunk_size = 800,\n",
    "        chunk_overlap  = 200,\n",
    "        length_function = len,\n",
    "    )\n",
    "    texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "    #step 4\n",
    "    embeddings = OpenAIEmbeddings(model= 'gpt-3.5-turbo-instruct')\n",
    "\n",
    "    #step 5\n",
    "    document_search = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "    return document_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates= ['candidate_1', 'candidate_2']\n",
    "Hr_question_1 = 'which companies have you worked in and for how long'\n",
    "Hr_question_2 = 'what are the python libraries you know.enumerate 3' \n",
    "Hr_question_3 = 'email and contact number'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build database to store \n",
    "df = pd.DataFrame(columns= [f'{Hr_question_1}',f'{Hr_question_2}' ,f'{Hr_question_3}'  ] , index= candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gernate FAISS embeding documnent search for each document\n",
    "document_search_1 = generate_text_from_pdf(pdf_file=r'C:\\Users\\gghotr01\\OneDrive - Teranet\\Code\\Langchain\\data\\CV_Jasprit_Labana_Ghotra.pdf')\n",
    "document_search_2 = generate_text_from_pdf(pdf_file=r'C:\\Users\\gghotr01\\OneDrive - Teranet\\Code\\Langchain\\data\\fake_cv.pdf')\n",
    "# run query on each for each candidate\n",
    "# update the dataframe as we process it \n",
    "document_search_list = [document_search_1 , document_search_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate, document_search  in zip(candidates,document_search_list) :\n",
    "    print(candidate , document_search)\n",
    "    for query in [Hr_question_1, Hr_question_2, Hr_question_3]:\n",
    "        print(query)\n",
    "        docs = document_search.similarity_search(query)\n",
    "        df.loc[candidate , query] = chain.run(input_documents=docs, question=query)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lanchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
